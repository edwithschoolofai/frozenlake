# 강화 학습

## OpenAI Gym 환경
### 새로운 환경 생성

다음 코드를 사용해서 새로운 환경을 생성합니다. 

```
import gym
import deeprl_hw1.envs

env = gym.make('Deterministic-4x4-FrozenLake-v0')
```

### 행동

네 가지 행동이 가능합니다. LEFT, UP, DOWN, RIGHT는 정수로 표현됩니다.
`deep_rl_hw1.envs` 에 이와 관련된 변수들이 선언되어 있습니다. 예를 들면,

```
print(deeprl_hw1.envs.LEFT)
```

는 0을 출력합니다.

### 환경 속성

이 클래스에는 다음과 같은 중요한 속성들이 있습니다. 

- `nS` :: 상태의 개수 
- `nA` :: 행동의 개수
- `P` :: 전이, 보상, 종단 상태

`P` 는 가치 순환법과 정책 순환법을 구현하는 데 가장 중요한 속성입니다. 
이 속성은 특정 맵 인스턴스의 모델을 가지고 있습니다.
다음과 같이 리스트에 대한 사전의 사전 형태로 되어있습니다.   

```
P[s][a] = [(prob, nextstate, reward, is_terminal), ...]
```

예를 들면, 상태 0에서 LEFT로 동작할 확률을 구할 때 다음과 같은 코드를 씁니다. 

```
env.P[0][deeprl_hw1.envs.LEFT]
```

위 코드는 `Deterministic-4x4-FrozenLake-v0` 환경에 대한 `[(1.0, 0, 0.0, False)]` 를 반환합니다. 
리스트 안에는 튜플이 한 개 있습니다. 따라서, 도달 가능한 다음 상태는 하나입니다. 
튜플 속 두 번째 값은 다음 상태가 상태 0이라는 것을 의미합니다.
튜플의 첫 번째 값은 이 상태가 다음 상태일 확률을 1이라는 것을 의미합니다. 
튜플의 세 번째 값은 이 (상태, 동작) 쌍의 보상 함수는 `R(0,LEFT) = 0` 이라는 것을 의미합니다. 
튜플의 마지막 값은 다음 상태는 종단 상태가 아니라는 것을 의미합니다. 

##
### 임의의 정책 실행하기

example.py에 어떤 환경에 대해서 임의의 정책을 실행하는 예시가 있습니다. 

#가치 순환법
여러 환경에 대한 최적 정책은 <environment>.py 파일들에 있습니다. 
